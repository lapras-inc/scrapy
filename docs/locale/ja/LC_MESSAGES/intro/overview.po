# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2008–2018, Scrapy developers
# This file is distributed under the same license as the Scrapy package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2018.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Scrapy 1.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2018-09-25 09:29+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../intro/overview.rst:5
msgid "Scrapy at a glance"
msgstr "Scrapyの概要"

#: ../../intro/overview.rst:7
msgid ""
"Scrapy is an application framework for crawling web sites and extracting "
"structured data which can be used for a wide range of useful "
"applications, like data mining, information processing or historical "
"archival."
msgstr ""
"Scrapyは、Webサイトをクロールし、データマイニング、情報処理、アーカイブなどの幅広い有用なアプリケーションに"
"使用できる構造化データを抽出するためのアプリケーションフレームワークです。"

#: ../../intro/overview.rst:11
msgid ""
"Even though Scrapy was originally designed for `web scraping`_, it can "
"also be used to extract data using APIs (such as `Amazon Associates Web "
"Services`_) or as a general purpose web crawler."
msgstr ""
"Scrapyはもともと `Webスクレイピング`_ 用に設計されていましたが、"
"API（`Amazon Associates Web Services`_ など）または汎用Webクローラーとしてデータを抽出するためにも使用できます。"

#: ../../intro/overview.rst:17
msgid "Walk-through of an example spider"
msgstr "Spiderサンプルのウォークスルー"

#: ../../intro/overview.rst:19
msgid ""
"In order to show you what Scrapy brings to the table, we'll walk you "
"through an example of a Scrapy Spider using the simplest way to run a "
"spider."
msgstr ""
"Scrapyが提供してくれるものを示すために、簡単なSpiderの実行例を紹介します。"

#: ../../intro/overview.rst:22
msgid ""
"Here's the code for a spider that scrapes famous quotes from website "
"http://quotes.toscrape.com, following the pagination::"
msgstr ""
"ウェブサイト http://quotes.toscrape.com からページネーションを辿って引用を抽出するSpiderのコードは次のとおりです："

#: ../../intro/overview.rst:46
msgid ""
"Put this in a text file, name it to something like ``quotes_spider.py`` "
"and run the spider using the :command:`runspider` command::"
msgstr ""
"このコードを ``quotes_spider.py`` といった名前で保存し、:command:`runspider` コマンドでSpiderを実行してください："

#: ../../intro/overview.rst:52
msgid ""
"When this finishes you will have in the ``quotes.json`` file a list of "
"the quotes in JSON format, containing text and author, looking like this "
"(reformatted here for better readability)::"
msgstr ""
"終了すると引用リストが ``quotes.json`` というJSON形式のファイルに保存されます。"
"これは次のような内容になります（読みやすくするために再フォーマットしています）。"

#: ../../intro/overview.rst:72
msgid "What just happened?"
msgstr "何か起きたのか？"

#: ../../intro/overview.rst:74
msgid ""
"When you ran the command ``scrapy runspider quotes_spider.py``, Scrapy "
"looked for a Spider definition inside it and ran it through its crawler "
"engine."
msgstr ""
"``scrapy runspider quotes_spider.py`` コマンドを実行すると、"
"Scrapyは該当のSpider定義を探して、クローラーエンジンを通して実行します。"

#: ../../intro/overview.rst:77
msgid ""
"The crawl started by making requests to the URLs defined in the "
"``start_urls`` attribute (in this case, only the URL for quotes in "
"*humor* category) and called the default callback method ``parse``, "
"passing the response object as an argument. In the ``parse`` callback, we"
" loop through the quote elements using a CSS Selector, yield a Python "
"dict with the extracted quote text and author, look for a link to the "
"next page and schedule another request using the same ``parse`` method as"
" callback."
msgstr ""
"クロールが開始すると、 ``start_urls`` 属性で定義されたURL（この場合は *humor* カテゴリーのURLのみ）にリクエストを行い、"
"responseオブジェクトを引数としてデフォルトのコールバックメソッド ``parse`` が呼び出されます。"
"``parse`` コールバックでは、CSSセレクタを使用して引用テキストの要素をループし、抽出された引用と作者でPythonのdictを生成し、"
"次のページへのリンクを探し、コールバックと同じ ``parse`` メソッドを使用して次ページへのリクエストをスケジューリングします。"

#: ../../intro/overview.rst:85
msgid ""
"Here you notice one of the main advantages about Scrapy: requests are "
":ref:`scheduled and processed asynchronously <topics-architecture>`.  "
"This means that Scrapy doesn't need to wait for a request to be finished "
"and processed, it can send another request or do other things in the "
"meantime. This also means that other requests can keep going even if some"
" request fails or an error happens while handling it."
msgstr ""
"ここにScrapyの利点の1つがあります。リクエストは :ref:`スケジューリングされ、非同期に処理されます <topics-architecture>` 。"
"つまり、リクエストが完了するのを待つ必要はなく、その間に別のリクエストを送信したり、他の処理を行うことができます。"
"これは、リクエストが失敗した場合や、処理中にエラーが発生した場合でも、他のリクエストが続行できることを意味します。"

#: ../../intro/overview.rst:92
msgid ""
"While this enables you to do very fast crawls (sending multiple "
"concurrent requests at the same time, in a fault-tolerant way) Scrapy "
"also gives you control over the politeness of the crawl through :ref:`a "
"few settings <topics-settings-ref>`. You can do things like setting a "
"download delay between each request, limiting amount of concurrent "
"requests per domain or per IP, and even :ref:`using an auto-throttling "
"extension <topics-autothrottle>` that tries to figure out these "
"automatically."
msgstr ""
"これにより、非常に高速なクロールが可能になります（複数の同時リクエストをフォールトトレラントな方法で送信できます）。"
"また、Scrapyを使用すると、 :ref:`いくつかの設定 <topics-settings-ref>` でクロールの礼儀正しさを制御できます。"
"各リクエストごとのダウンロード遅延を設定したり、ドメインまたはIPごとに平行リクエストの量を制限したり、"
":ref:`クロール速度を自動的に調整する拡張機能を使用する <topics-autothrottle>` ことができます。"

#: ../../intro/overview.rst:102
msgid ""
"This is using :ref:`feed exports <topics-feed-exports>` to generate the "
"JSON file, you can easily change the export format (XML or CSV, for "
"example) or the storage backend (FTP or `Amazon S3`_, for example).  You "
"can also write an :ref:`item pipeline <topics-item-pipeline>` to store "
"the items in a database."
msgstr ""
":ref:`フィードのエクスポート <topics-feed-exports>` を使用してJSONファイルを生成したり、"
"エクスポート形式（XMLやCSVなど）やストレージバックエンド（FTPや `Amazon S3`_ など）を簡単に変更することができます。"
":ref:`Itemパイプライン <topics-item-pipeline>` を作成してItemをデータベースに格納することもできます。"

#: ../../intro/overview.rst:111
msgid "What else?"
msgstr "それ以外には？"

#: ../../intro/overview.rst:113
msgid ""
"You've seen how to extract and store items from a website using Scrapy, "
"but this is just the surface. Scrapy provides a lot of powerful features "
"for making scraping easy and efficient, such as:"
msgstr ""
"ここまで、Scrapyを使用してウェブサイトからアイテムを抽出して保存する方法を見てきましたが、これはほんの一部に過ぎません。"
"Scrapyは、簡単で効率的なスクレイピングを作成するための強力な機能を、以下のように多数提供しています。"

#: ../../intro/overview.rst:117
msgid ""
"Built-in support for :ref:`selecting and extracting <topics-selectors>` "
"data from HTML/XML sources using extended CSS selectors and XPath "
"expressions, with helper methods to extract using regular expressions."
msgstr ""
"拡張CSSセレクタとXPath式を使用し、正規表現を使用して抽出するヘルパーメソッドを併用して、"
"HTML/XMLのソースからデータを :ref:`選択および抽出 <topics-selectors>` する組み込みのサポート。"

#: ../../intro/overview.rst:121
msgid ""
"An :ref:`interactive shell console <topics-shell>` (IPython aware) for "
"trying out the CSS and XPath expressions to scrape data, very useful when"
" writing or debugging your spiders."
msgstr ""
"CSSやXPath式を試してデータを抽出するための :ref:`インタラクティブなシェル <topics-shell>` （IPython対応）。"
"Spiderの作成やデバッグに非常に便利です。"

#: ../../intro/overview.rst:125
msgid ""
"Built-in support for :ref:`generating feed exports <topics-feed-exports>`"
" in multiple formats (JSON, CSV, XML) and storing them in multiple "
"backends (FTP, S3, local filesystem)"
msgstr ""
"複数の形式（JSON, CSV, XML）で :ref:`フィードのエクスポート <topics-feed-exports>` を生成し、"
"それらを複数のバックエンド（FTP、S3、ローカルファイルシステム）に格納するための組み込みサポート。"

#: ../../intro/overview.rst:129
msgid ""
"Robust encoding support and auto-detection, for dealing with foreign, "
"non-standard and broken encoding declarations."
msgstr ""
"国外、非標準、壊れたエンコーディング宣言を扱うための強力なエンコーディングのサポートと自動検出。"

#: ../../intro/overview.rst:132
msgid ""
":ref:`Strong extensibility support <extending-scrapy>`, allowing you to "
"plug in your own functionality using :ref:`signals <topics-signals>` and "
"a well-defined API (middlewares, :ref:`extensions <topics-extensions>`, "
"and :ref:`pipelines <topics-item-pipeline>`)."
msgstr ""
":ref:`強力な拡張性 <extending-scrapy>` により、 :ref:`シグナル <topics-signals>` と"
"明確に定義されたAPI（ミドルウェア、 :ref:`拡張機能 <topics-extensions>` 、 :ref:`パイプライン <topics-item-pipeline>` ）を"
"使用して、プラグインとして独自の機能を追加することができます。"

#: ../../intro/overview.rst:137
msgid "Wide range of built-in extensions and middlewares for handling:"
msgstr "幅広い組み込み拡張機能とハンドリング用のミドルウェア："

#: ../../intro/overview.rst:139
msgid "cookies and session handling"
msgstr "クッキーとセッション処理"

#: ../../intro/overview.rst:140
msgid "HTTP features like compression, authentication, caching"
msgstr "圧縮、認証、キャッシュなどのHTTP機能"

#: ../../intro/overview.rst:141
msgid "user-agent spoofing"
msgstr "ユーザーエージェントのなりすまし"

#: ../../intro/overview.rst:142
msgid "robots.txt"
msgstr ""

#: ../../intro/overview.rst:143
msgid "crawl depth restriction"
msgstr "クロール深度の制限"

#: ../../intro/overview.rst:144
msgid "and more"
msgstr "その他"

#: ../../intro/overview.rst:146
msgid ""
"A :ref:`Telnet console <topics-telnetconsole>` for hooking into a Python "
"console running inside your Scrapy process, to introspect and debug your "
"crawler"
msgstr ""
"Scrapyプロセス内で動作するPythonコンソールにフックするための :ref:`Telnetコンソール <topics-telnetconsole>` 、"
"クローラの内部調査とデバッグ"

#: ../../intro/overview.rst:150
msgid ""
"Plus other goodies like reusable spiders to crawl sites from `Sitemaps`_ "
"and XML/CSV feeds, a media pipeline for :ref:`automatically downloading "
"images <topics-media-pipeline>` (or any other media) associated with the "
"scraped items, a caching DNS resolver, and much more!"
msgstr ""
"さらに、`サイトマップ`_ やXML/CSVフィードからWebサイトをクロールするための再利用可能なSpider、"
"抽出したアイテムに関連付けられた画像（または他のメディア）を :ref:`自動的にダウンロード <topics-media-pipeline>` するメディアパイプライン、"
"DNSリゾルバのキャッシュなど、他にもたくさんの機能があります。"

#: ../../intro/overview.rst:156
msgid "What's next?"
msgstr "次のステップは？"

#: ../../intro/overview.rst:158
msgid ""
"The next steps for you are to :ref:`install Scrapy <intro-install>`, "
":ref:`follow through the tutorial <intro-tutorial>` to learn how to "
"create a full-blown Scrapy project and `join the community`_. Thanks for "
"your interest!"
msgstr ""
"次のステップは、 :ref:`Scrapyをインストール <intro-install>` して、"
":ref:`チュートリアル <intro-tutorial>` に従って本格的なScrapyプロジェクトを作成し、"
"そして `コミュニティに参加する`_ ことです。興味を持って頂いて感謝します！"
