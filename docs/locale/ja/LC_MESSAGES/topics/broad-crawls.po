# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2008â€“2018, Scrapy developers
# This file is distributed under the same license as the Scrapy package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2018.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Scrapy 1.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2018-09-25 09:29+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.6.0\n"

#: ../../topics/broad-crawls.rst:5
msgid "Broad Crawls"
msgstr ""

#: ../../topics/broad-crawls.rst:7
msgid ""
"Scrapy defaults are optimized for crawling specific sites. These sites "
"are often handled by a single Scrapy spider, although this is not "
"necessary or required (for example, there are generic spiders that handle"
" any given site thrown at them)."
msgstr ""

#: ../../topics/broad-crawls.rst:12
msgid ""
"In addition to this \"focused crawl\", there is another common type of "
"crawling which covers a large (potentially unlimited) number of domains, "
"and is only limited by time or other arbitrary constraint, rather than "
"stopping when the domain was crawled to completion or when there are no "
"more requests to perform. These are called \"broad crawls\" and is the "
"typical crawlers employed by search engines."
msgstr ""

#: ../../topics/broad-crawls.rst:19
msgid "These are some common properties often found in broad crawls:"
msgstr ""

#: ../../topics/broad-crawls.rst:21
msgid ""
"they crawl many domains (often, unbounded) instead of a specific set of "
"sites"
msgstr ""

#: ../../topics/broad-crawls.rst:23
msgid ""
"they don't necessarily crawl domains to completion, because it would be "
"impractical (or impossible) to do so, and instead limit the crawl by time"
" or number of pages crawled"
msgstr ""

#: ../../topics/broad-crawls.rst:27
msgid ""
"they are simpler in logic (as opposed to very complex spiders with many "
"extraction rules) because data is often post-processed in a separate "
"stage"
msgstr ""

#: ../../topics/broad-crawls.rst:30
msgid ""
"they crawl many domains concurrently, which allows them to achieve faster"
" crawl speeds by not being limited by any particular site constraint "
"(each site is crawled slowly to respect politeness, but many sites are "
"crawled in parallel)"
msgstr ""

#: ../../topics/broad-crawls.rst:35
msgid ""
"As said above, Scrapy default settings are optimized for focused crawls, "
"not broad crawls. However, due to its asynchronous architecture, Scrapy "
"is very well suited for performing fast broad crawls. This page "
"summarizes some things you need to keep in mind when using Scrapy for "
"doing broad crawls, along with concrete suggestions of Scrapy settings to"
" tune in order to achieve an efficient broad crawl."
msgstr ""

#: ../../topics/broad-crawls.rst:43
msgid "Increase concurrency"
msgstr ""

#: ../../topics/broad-crawls.rst:45
msgid ""
"Concurrency is the number of requests that are processed in parallel. "
"There is a global limit and a per-domain limit."
msgstr ""

#: ../../topics/broad-crawls.rst:48
msgid ""
"The default global concurrency limit in Scrapy is not suitable for "
"crawling many different domains in parallel, so you will want to increase"
" it. How much to increase it will depend on how much CPU you crawler will"
" have available. A good starting point is ``100``, but the best way to "
"find out is by doing some trials and identifying at what concurrency your"
" Scrapy process gets CPU bounded. For optimum performance, you should "
"pick a concurrency where CPU usage is at 80-90%."
msgstr ""

#: ../../topics/broad-crawls.rst:56
msgid "To increase the global concurrency use::"
msgstr ""

#: ../../topics/broad-crawls.rst:61
msgid "Increase Twisted IO thread pool maximum size"
msgstr ""

#: ../../topics/broad-crawls.rst:63
msgid ""
"Currently Scrapy does DNS resolution in a blocking way with usage of "
"thread pool. With higher concurrency levels the crawling could be slow or"
" even fail hitting DNS resolver timeouts. Possible solution to increase "
"the number of threads handling DNS queries. The DNS queue will be "
"processed faster speeding up establishing of connection and crawling "
"overall."
msgstr ""

#: ../../topics/broad-crawls.rst:69
msgid "To increase maximum thread pool size use::"
msgstr ""

#: ../../topics/broad-crawls.rst:74
msgid "Setup your own DNS"
msgstr ""

#: ../../topics/broad-crawls.rst:76
msgid ""
"If you have multiple crawling processes and single central DNS, it can "
"act like DoS attack on the DNS server resulting to slow down of entire "
"network or even blocking your machines. To avoid this setup your own DNS "
"server with local cache and upstream to some large DNS like OpenDNS or "
"Verizon."
msgstr ""

#: ../../topics/broad-crawls.rst:82
msgid "Reduce log level"
msgstr ""

#: ../../topics/broad-crawls.rst:84
msgid ""
"When doing broad crawls you are often only interested in the crawl rates "
"you get and any errors found. These stats are reported by Scrapy when "
"using the ``INFO`` log level. In order to save CPU (and log storage "
"requirements) you should not use ``DEBUG`` log level when preforming "
"large broad crawls in production. Using ``DEBUG`` level when developing "
"your (broad) crawler may be fine though."
msgstr ""

#: ../../topics/broad-crawls.rst:91
msgid "To set the log level use::"
msgstr ""

#: ../../topics/broad-crawls.rst:96
msgid "Disable cookies"
msgstr ""

#: ../../topics/broad-crawls.rst:98
msgid ""
"Disable cookies unless you *really* need. Cookies are often not needed "
"when doing broad crawls (search engine crawlers ignore them), and they "
"improve performance by saving some CPU cycles and reducing the memory "
"footprint of your Scrapy crawler."
msgstr ""

#: ../../topics/broad-crawls.rst:103
msgid "To disable cookies use::"
msgstr ""

#: ../../topics/broad-crawls.rst:108
msgid "Disable retries"
msgstr ""

#: ../../topics/broad-crawls.rst:110
msgid ""
"Retrying failed HTTP requests can slow down the crawls substantially, "
"specially when sites causes are very slow (or fail) to respond, thus "
"causing a timeout error which gets retried many times, unnecessarily, "
"preventing crawler capacity to be reused for other domains."
msgstr ""

#: ../../topics/broad-crawls.rst:115
msgid "To disable retries use::"
msgstr ""

#: ../../topics/broad-crawls.rst:120
msgid "Reduce download timeout"
msgstr ""

#: ../../topics/broad-crawls.rst:122
msgid ""
"Unless you are crawling from a very slow connection (which shouldn't be "
"the case for broad crawls) reduce the download timeout so that stuck "
"requests are discarded quickly and free up capacity to process the next "
"ones."
msgstr ""

#: ../../topics/broad-crawls.rst:126
msgid "To reduce the download timeout use::"
msgstr ""

#: ../../topics/broad-crawls.rst:131
msgid "Disable redirects"
msgstr ""

#: ../../topics/broad-crawls.rst:133
msgid ""
"Consider disabling redirects, unless you are interested in following "
"them. When doing broad crawls it's common to save redirects and resolve "
"them when revisiting the site at a later crawl. This also help to keep "
"the number of request constant per crawl batch, otherwise redirect loops "
"may cause the crawler to dedicate too many resources on any specific "
"domain."
msgstr ""

#: ../../topics/broad-crawls.rst:139
msgid "To disable redirects use::"
msgstr ""

#: ../../topics/broad-crawls.rst:144
msgid "Enable crawling of \"Ajax Crawlable Pages\""
msgstr ""

#: ../../topics/broad-crawls.rst:146
msgid ""
"Some pages (up to 1%, based on empirical data from year 2013) declare "
"themselves as `ajax crawlable`_. This means they provide plain HTML "
"version of content that is usually available only via AJAX. Pages can "
"indicate it in two ways:"
msgstr ""

#: ../../topics/broad-crawls.rst:151
msgid "by using ``#!`` in URL - this is the default way;"
msgstr ""

#: ../../topics/broad-crawls.rst:152
msgid ""
"by using a special meta tag - this way is used on \"main\", \"index\" "
"website pages."
msgstr ""

#: ../../topics/broad-crawls.rst:155
msgid ""
"Scrapy handles (1) automatically; to handle (2) enable "
":ref:`AjaxCrawlMiddleware <ajaxcrawl-middleware>`::"
msgstr ""

#: ../../topics/broad-crawls.rst:160
msgid ""
"When doing broad crawls it's common to crawl a lot of \"index\" web "
"pages; AjaxCrawlMiddleware helps to crawl them correctly. It is turned "
"OFF by default because it has some performance overhead, and enabling it "
"for focused crawls doesn't make much sense."
msgstr ""

